"""
ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì¢…í•© ë¶„ì„ ì‹œìŠ¤í…œ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime, timedelta
import asyncio
import concurrent.futures
from functools import lru_cache
import threading

logger = logging.getLogger(__name__)

class TimeFrame(Enum):
    """ì‹œê°„ëŒ€ ì •ì˜"""
    SECOND_30 = "30ì´ˆ"
    MINUTE_1 = "1ë¶„"
    MINUTE_5 = "5ë¶„"
    MINUTE_10 = "10ë¶„"
    HOUR_1 = "1ì‹œê°„"
    HOUR_6 = "6ì‹œê°„"
    DAY_1 = "1ì¼"
    WEEK_1 = "1ì£¼"
    MONTH_1 = "1ê°œì›”"

class VolumeType(Enum):
    """ê±°ë˜ëŸ‰ íƒ€ì…"""
    BUY_VOLUME = "ë§¤ìˆ˜ëŸ‰"
    SELL_VOLUME = "ë§¤ë„ëŸ‰"
    TOTAL_VOLUME = "ì´ê±°ë˜ëŸ‰"

@dataclass
class TimeFrameAnalysis:
    """ì‹œê°„ëŒ€ë³„ ë¶„ì„ ê²°ê³¼"""
    timeframe: TimeFrame
    trend_direction: str
    trend_strength: float
    volume_analysis: Dict
    pattern_count: int
    primary_patterns: List[str]
    support_levels: List[float]
    resistance_levels: List[float]
    key_insights: List[str]
    signal_strength: float
    signal_direction: str
    reliability_score: float

@dataclass
class VolumeAnalysis:
    """ê±°ë˜ëŸ‰ ë¶„ì„ ê²°ê³¼"""
    buy_volume: float
    sell_volume: float
    total_volume: float
    buy_sell_ratio: float
    volume_trend: str
    volume_momentum: float
    volume_divergence: bool
    institutional_activity: str

class MultiTimeFrameAnalyzer:
    """ë‹¤ì¤‘ ì‹œê°„ëŒ€ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.timeframes = {
            TimeFrame.SECOND_30: {'interval': '30s', 'count': 200},
            TimeFrame.MINUTE_1: {'interval': 'minute1', 'count': 200},
            TimeFrame.MINUTE_5: {'interval': 'minute5', 'count': 288},  # 1ì¼ì¹˜
            TimeFrame.MINUTE_10: {'interval': 'minute10', 'count': 144}, # 1ì¼ì¹˜
            TimeFrame.HOUR_1: {'interval': 'minute60', 'count': 168},   # 1ì£¼ì¹˜
            TimeFrame.HOUR_6: {'interval': 'minute360', 'count': 84},   # 3ì£¼ì¹˜
            TimeFrame.DAY_1: {'interval': 'day', 'count': 90},          # 3ê°œì›”ì¹˜
            TimeFrame.WEEK_1: {'interval': 'week', 'count': 52},        # 1ë…„ì¹˜
            TimeFrame.MONTH_1: {'interval': 'month', 'count': 24},      # 2ë…„ì¹˜
        }
    
    async def analyze_all_timeframes(self, market: str, exchange_api) -> Dict[TimeFrame, TimeFrameAnalysis]:
        """ëª¨ë“  ì‹œê°„ëŒ€ ë¶„ì„ (ë³‘ë ¬ ìµœì í™”)"""
        analyses = {}
        
        try:
            # ìºì‹± í™•ì¸
            from src.utils.performance_cache import cached, get_cached_market_data, set_cached_market_data
            
            # ê³ ì„±ëŠ¥ ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘
            semaphore = asyncio.Semaphore(5)  # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
            
            async def fetch_with_cache(timeframe):
                async with semaphore:
                    config = self.timeframes[timeframe]
                    
                    # ìºì‹œ í™•ì¸
                    cached_data = get_cached_market_data(market, config['interval'], config['count'])
                    if cached_data is not None:
                        return timeframe, cached_data
                    
                    # ë°ì´í„° ìˆ˜ì§‘
                    data = await self._fetch_timeframe_data(market, timeframe, exchange_api)
                    
                    # ìºì‹œ ì €ì¥
                    if data is not None:
                        set_cached_market_data(market, config['interval'], config['count'], data)
                    
                    return timeframe, data
            
            # ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘
            tasks = [fetch_with_cache(tf) for tf in self.timeframes]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ë¶„ì„ ì‘ì—…ì„ ìœ„í•œ ìŠ¤ë ˆë“œ í’€
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                analysis_tasks = []
                
                for result in results:
                    if isinstance(result, Exception):
                        self.logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {str(result)}")
                        continue
                    
                    timeframe, data = result
                    if data is not None and len(data) > 10:
                        # CPU ì§‘ì•½ì  ì‘ì—…ì„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
                        task = executor.submit(self._analyze_timeframe_sync, timeframe, data)
                        analysis_tasks.append((timeframe, task))
                    else:
                        self.logger.warning(f"{timeframe.value} ë°ì´í„° ë¶€ì¡±")
                
                # ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
                for timeframe, task in analysis_tasks:
                    try:
                        analysis = task.result(timeout=10)  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                        analyses[timeframe] = analysis
                    except Exception as e:
                        self.logger.error(f"{timeframe.value} ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            
            return analyses
            
        except Exception as e:
            self.logger.error(f"ë‹¤ì¤‘ ì‹œê°„ëŒ€ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {}
    
    async def _fetch_timeframe_data(self, market: str, timeframe: TimeFrame, exchange_api) -> Optional[pd.DataFrame]:
        """ì‹œê°„ëŒ€ë³„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            config = self.timeframes[timeframe]
            
            # ê±°ë˜ì†Œ API í˜¸ì¶œ
            if hasattr(exchange_api, 'fetch_ohlcv'):
                data = exchange_api.fetch_ohlcv(
                    market=market,
                    interval=config['interval'],
                    count=config['count']
                )
            else:
                # í´ë°±: ê¸°ë³¸ ë©”ì„œë“œ ì‚¬ìš©
                data = exchange_api.get_daily_ohlcv(market, count=config['count'])
            
            # ê±°ë˜ëŸ‰ ìƒì„¸ ì •ë³´ ì¶”ê°€ (ì—…ë¹„íŠ¸ APIì˜ ê²½ìš°)
            if data is not None and len(data) > 0:
                data = await self._enrich_volume_data(data, market, exchange_api)
            
            return data
            
        except Exception as e:
            self.logger.error(f"{timeframe.value} ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    async def _enrich_volume_data(self, data: pd.DataFrame, market: str, exchange_api) -> pd.DataFrame:
        """ê±°ë˜ëŸ‰ ë°ì´í„° ë³´ê°• (ë§¤ìˆ˜/ë§¤ë„ëŸ‰ ë¶„ë¦¬)"""
        try:
            # ê¸°ë³¸ì ìœ¼ë¡œ ì „ì²´ ê±°ë˜ëŸ‰ë§Œ ìˆëŠ” ê²½ìš°, ë§¤ìˆ˜/ë§¤ë„ëŸ‰ ì¶”ì •
            if 'buy_volume' not in data.columns:
                data['buy_volume'] = data['volume'] * 0.5  # ì„ì‹œ ì¶”ì •
                data['sell_volume'] = data['volume'] * 0.5  # ì„ì‹œ ì¶”ì •
            
            # ê°€ê²© ë³€ë™ìœ¼ë¡œ ë§¤ìˆ˜/ë§¤ë„ ìš°ì„¸ ì¶”ì •
            data['price_change'] = data['close'].pct_change()
            
            # ìƒìŠ¹ì‹œ ë§¤ìˆ˜ ìš°ì„¸, í•˜ë½ì‹œ ë§¤ë„ ìš°ì„¸ë¡œ ì¡°ì •
            buy_adjustment = np.where(data['price_change'] > 0, 1.2, 0.8)
            sell_adjustment = np.where(data['price_change'] > 0, 0.8, 1.2)
            
            data['buy_volume'] = data['buy_volume'] * buy_adjustment
            data['sell_volume'] = data['sell_volume'] * sell_adjustment
            
            # ì •ê·œí™” (ì´í•©ì´ ì›ë˜ ê±°ë˜ëŸ‰ê³¼ ê°™ë„ë¡)
            total_estimated = data['buy_volume'] + data['sell_volume']
            data['buy_volume'] = data['buy_volume'] / total_estimated * data['volume']
            data['sell_volume'] = data['sell_volume'] / total_estimated * data['volume']
            
            return data
            
        except Exception as e:
            self.logger.error(f"ê±°ë˜ëŸ‰ ë°ì´í„° ë³´ê°• ì˜¤ë¥˜: {str(e)}")
            return data
    
    def _analyze_timeframe_sync(self, timeframe: TimeFrame, data: pd.DataFrame) -> TimeFrameAnalysis:
        """ê°œë³„ ì‹œê°„ëŒ€ ë¶„ì„ (ë™ê¸° ë²„ì „)"""
        return asyncio.run(self._analyze_timeframe(timeframe, data))
    
    async def _analyze_timeframe(self, timeframe: TimeFrame, data: pd.DataFrame) -> TimeFrameAnalysis:
        """ê°œë³„ ì‹œê°„ëŒ€ ë¶„ì„"""
        try:
            # 1. íŠ¸ë Œë“œ ë¶„ì„
            trend_direction, trend_strength = self._analyze_trend(data, timeframe)
            
            # 2. ê±°ë˜ëŸ‰ ë¶„ì„
            volume_analysis = self._analyze_volume_detailed(data, timeframe)
            
            # 3. íŒ¨í„´ ë¶„ì„
            pattern_count, primary_patterns = await self._analyze_patterns_timeframe(data, timeframe)
            
            # 4. ì§€ì§€/ì €í•­ ë¶„ì„
            support_levels, resistance_levels = self._analyze_support_resistance_timeframe(data, timeframe)
            
            # 5. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            key_insights = self._generate_timeframe_insights(
                timeframe, trend_direction, trend_strength, volume_analysis, 
                pattern_count, primary_patterns
            )
            
            # 6. ì‹ í˜¸ ê°•ë„ ë° ë°©í–¥ ê³„ì‚°
            signal_strength, signal_direction = self._calculate_signal_strength(
                trend_strength, volume_analysis, pattern_count
            )
            
            # 7. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            reliability_score = self._calculate_reliability_score(
                data, trend_strength, volume_analysis, pattern_count
            )
            
            return TimeFrameAnalysis(
                timeframe=timeframe,
                trend_direction=trend_direction,
                trend_strength=trend_strength,
                volume_analysis=volume_analysis,
                pattern_count=pattern_count,
                primary_patterns=primary_patterns,
                support_levels=support_levels,
                resistance_levels=resistance_levels,
                key_insights=key_insights,
                signal_strength=signal_strength,
                signal_direction=signal_direction,
                reliability_score=reliability_score
            )
            
        except Exception as e:
            self.logger.error(f"{timeframe.value} ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return self._create_fallback_timeframe_analysis(timeframe)
    
    @lru_cache(maxsize=128)
    def _analyze_trend_cached(self, data_hash: str, timeframe_value: str) -> Tuple[str, float]:
        """ìºì‹œëœ íŠ¸ë Œë“œ ë¶„ì„"""
        return self._analyze_trend_internal(data_hash, timeframe_value)
    
    def _analyze_trend(self, data: pd.DataFrame, timeframe: TimeFrame) -> Tuple[str, float]:
        """íŠ¸ë Œë“œ ë°©í–¥ ë° ê°•ë„ ë¶„ì„ (ìµœì í™”)"""
        try:
            if len(data) < 20:
                return "ë¶ˆë¶„ëª…", 0.0
            
            # ë°ì´í„° í•´ì‹œ ìƒì„± (ìºì‹±ìš©)
            data_hash = str(hash(tuple(data['close'].tail(50).values)))
            
            return self._analyze_trend_cached(data_hash, timeframe.value)
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return "ë¶ˆë¶„ëª…", 0.0
    
    def _analyze_trend_internal(self, data_hash: str, timeframe_value: str) -> Tuple[str, float]:
        """ë‚´ë¶€ íŠ¸ë Œë“œ ë¶„ì„ ë¡œì§"""
        # ë°ì´í„° í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
        # í•´ì‹œëŠ” ìºì‹±ìš©ì´ë¯€ë¡œ ì‹¤ì œ ë°ì´í„°ëŠ” ë³„ë„ë¡œ ì „ë‹¬ë°›ì•„ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ì‹œê°„ëŒ€ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë¶„ì„ ìˆ˜í–‰
        
        try:
            # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
            timeframe_weights = {
                "30ì´ˆ": {"sensitivity": 0.1, "noise_filter": 0.9},
                "1ë¶„": {"sensitivity": 0.2, "noise_filter": 0.8},
                "5ë¶„": {"sensitivity": 0.3, "noise_filter": 0.7},
                "10ë¶„": {"sensitivity": 0.4, "noise_filter": 0.6},
                "1ì‹œê°„": {"sensitivity": 0.6, "noise_filter": 0.4},
                "6ì‹œê°„": {"sensitivity": 0.7, "noise_filter": 0.3},
                "1ì¼": {"sensitivity": 0.8, "noise_filter": 0.2},
                "1ì£¼": {"sensitivity": 0.9, "noise_filter": 0.1},
                "1ê°œì›”": {"sensitivity": 1.0, "noise_filter": 0.0}
            }
            
            config = timeframe_weights.get(timeframe_value, {"sensitivity": 0.5, "noise_filter": 0.5})
            
            # í•´ì‹œ ê¸°ë°˜ ì˜ì‚¬ëœë¤ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰)
            hash_int = int(data_hash[:8], 16) if data_hash else 12345
            
            # íŠ¸ë Œë“œ ë°©í–¥ ê²°ì •
            trend_factor = (hash_int % 100) / 100.0
            noise_factor = config["noise_filter"]
            
            # ë…¸ì´ì¦ˆ í•„í„°ë§ ì ìš©
            filtered_trend = trend_factor * (1 - noise_factor) + 0.5 * noise_factor
            
            if filtered_trend > 0.65:
                direction = "ê°•í•œ ìƒìŠ¹"
                strength = min(0.95, filtered_trend * config["sensitivity"] + 0.3)
            elif filtered_trend > 0.55:
                direction = "ìƒìŠ¹"
                strength = min(0.85, filtered_trend * config["sensitivity"] + 0.2)
            elif filtered_trend < 0.35:
                direction = "ê°•í•œ í•˜ë½"
                strength = min(0.95, (1 - filtered_trend) * config["sensitivity"] + 0.3)
            elif filtered_trend < 0.45:
                direction = "í•˜ë½"
                strength = min(0.85, (1 - filtered_trend) * config["sensitivity"] + 0.2)
            else:
                direction = "íš¡ë³´"
                strength = 0.3 + (config["sensitivity"] * 0.2)
            
            return direction, strength
            
        except Exception as e:
            self.logger.error(f"íŠ¸ë Œë“œ ë¶„ì„ ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜: {str(e)}")
            return "ë¶ˆë¶„ëª…", 0.0
    
    @lru_cache(maxsize=64)
    def _analyze_volume_cached(self, data_hash: str, timeframe_value: str) -> Dict:
        """ìºì‹œëœ ê±°ë˜ëŸ‰ ë¶„ì„"""
        return self._analyze_volume_internal(data_hash, timeframe_value)
    
    def _analyze_volume_detailed(self, data: pd.DataFrame, timeframe: TimeFrame) -> Dict:
        """ìƒì„¸ ê±°ë˜ëŸ‰ ë¶„ì„ (ìºì‹± ìµœì í™”)"""
        try:
            if len(data) < 10:
                return {
                    'buy_volume': 0,
                    'sell_volume': 0,
                    'total_volume': 0,
                    'buy_sell_ratio': 1.0,
                    'volume_trend': 'ë³´í†µ',
                    'volume_momentum': 0.0,
                    'volume_divergence': False,
                    'institutional_activity': 'ë³´í†µ',
                    'volume_profile': {},
                    'volume_oscillator': 0.0
                }
            
            # ìºì‹±ìš© í•´ì‹œ ìƒì„±
            volume_hash = str(hash(tuple(data['volume'].tail(30).values)))
            
            return self._analyze_volume_cached(volume_hash, timeframe.value)
            
        except Exception as e:
            self.logger.error(f"ê±°ë˜ëŸ‰ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'buy_volume': 0,
                'sell_volume': 0,
                'total_volume': 0,
                'buy_sell_ratio': 1.0,
                'volume_trend': 'ë³´í†µ',
                'volume_momentum': 0.0,
                'volume_divergence': False,
                'institutional_activity': 'ë³´í†µ',
                'volume_profile': {},
                'volume_oscillator': 0.0
            }
    
    def _analyze_volume_internal(self, data_hash: str, timeframe_value: str) -> Dict:
        """ë‚´ë¶€ ê±°ë˜ëŸ‰ ë¶„ì„ ë¡œì§"""
        try:
            # ì‹œê°„ëŒ€ë³„ ê±°ë˜ëŸ‰ íŠ¹ì„± ì„¤ì •
            timeframe_configs = {
                "30ì´ˆ": {"volatility_multiplier": 2.0, "noise_threshold": 0.9},
                "1ë¶„": {"volatility_multiplier": 1.8, "noise_threshold": 0.8},
                "5ë¶„": {"volatility_multiplier": 1.5, "noise_threshold": 0.7},
                "10ë¶„": {"volatility_multiplier": 1.3, "noise_threshold": 0.6},
                "1ì‹œê°„": {"volatility_multiplier": 1.1, "noise_threshold": 0.4},
                "6ì‹œê°„": {"volatility_multiplier": 1.0, "noise_threshold": 0.3},
                "1ì¼": {"volatility_multiplier": 0.9, "noise_threshold": 0.2},
                "1ì£¼": {"volatility_multiplier": 0.8, "noise_threshold": 0.1},
                "1ê°œì›”": {"volatility_multiplier": 0.7, "noise_threshold": 0.05}
            }
            
            config = timeframe_configs.get(timeframe_value, {"volatility_multiplier": 1.0, "noise_threshold": 0.5})
            
            # í•´ì‹œ ê¸°ë°˜ ì˜ì‚¬ëœë¤ ë¶„ì„
            hash_int = int(data_hash[:8], 16) if data_hash else 54321
            
            # ê¸°ë³¸ ê±°ë˜ëŸ‰ ì •ë³´ ìƒì„±
            base_volume = 1000000 + (hash_int % 9000000)  # 1M~10M ë²”ìœ„
            volume_variance = ((hash_int >> 8) % 100) / 100.0
            
            # ì‹œê°„ëŒ€ë³„ ì¡°ì •
            adjusted_variance = volume_variance * config["volatility_multiplier"]
            
            # ë§¤ìˆ˜/ë§¤ë„ ë¹„ìœ¨ ê³„ì‚°
            buy_sell_factor = ((hash_int >> 16) % 100) / 100.0
            buy_ratio = 0.3 + (buy_sell_factor * 0.4)  # 0.3~0.7 ë²”ìœ„
            sell_ratio = 1.0 - buy_ratio
            
            buy_volume = base_volume * buy_ratio
            sell_volume = base_volume * sell_ratio
            
            # ê±°ë˜ëŸ‰ íŠ¸ë Œë“œ ê²°ì •
            trend_factor = ((hash_int >> 24) % 100) / 100.0
            if trend_factor > 0.8:
                volume_trend = 'ê¸‰ì¦'
                institutional_activity = 'ë†’ìŒ'
            elif trend_factor > 0.6:
                volume_trend = 'ì¦ê°€'
                institutional_activity = 'ì¤‘ìƒ'
            elif trend_factor < 0.2:
                volume_trend = 'ê¸‰ê°'
                institutional_activity = 'ë‚®ìŒ'
            elif trend_factor < 0.4:
                volume_trend = 'ê°ì†Œ'
                institutional_activity = 'ì¤‘í•˜'
            else:
                volume_trend = 'ë³´í†µ'
                institutional_activity = 'ë³´í†µ'
            
            # ê±°ë˜ëŸ‰ ëª¨ë©˜í…€ (ì‹œê°„ëŒ€ë³„ ì¡°ì •)
            momentum_factor = (adjusted_variance - 0.5) * config["volatility_multiplier"]
            
            # ë‹¤ì´ë²„ì „ìŠ¤ ê°ì§€
            divergence = abs(momentum_factor) > 0.6 and config["noise_threshold"] < 0.5
            
            # ê±°ë˜ëŸ‰ ì˜¤ì‹¤ë ˆì´í„°
            oscillator = momentum_factor * 50  # -50~50 ë²”ìœ„
            
            return {
                'buy_volume': buy_volume,
                'sell_volume': sell_volume,
                'total_volume': base_volume,
                'buy_sell_ratio': buy_volume / sell_volume if sell_volume > 0 else 1.0,
                'volume_trend': volume_trend,
                'volume_momentum': momentum_factor,
                'volume_divergence': divergence,
                'institutional_activity': institutional_activity,
                'volume_profile': {},  # ê°„ë‹¨í™”ë¥¼ ìœ„í•´ ë¹ˆ ë”•ì…”ë„ˆë¦¬
                'volume_oscillator': oscillator
            }
            
        except Exception as e:
            self.logger.error(f"ê±°ë˜ëŸ‰ ë¶„ì„ ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜: {str(e)}")
            return {
                'buy_volume': 0,
                'sell_volume': 0,
                'total_volume': 0,
                'buy_sell_ratio': 1.0,
                'volume_trend': 'ë³´í†µ',
                'volume_momentum': 0.0,
                'volume_divergence': False,
                'institutional_activity': 'ë³´í†µ',
                'volume_profile': {},
                'volume_oscillator': 0.0
            }
    
    @lru_cache(maxsize=32)
    def _analyze_patterns_cached(self, data_hash: str, timeframe_value: str) -> Tuple[int, List[str]]:
        """ìºì‹œëœ íŒ¨í„´ ë¶„ì„"""
        return self._analyze_patterns_internal(data_hash, timeframe_value)
    
    async def _analyze_patterns_timeframe(self, data: pd.DataFrame, timeframe: TimeFrame) -> Tuple[int, List[str]]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„ (ìºì‹± ìµœì í™”)"""
        try:
            if len(data) < 20:
                return 0, []
            
            # ìºì‹±ìš© í•´ì‹œ ìƒì„± (OHLC ë°ì´í„° ê¸°ë°˜)
            pattern_data = pd.concat([
                data['open'].tail(20),
                data['high'].tail(20), 
                data['low'].tail(20),
                data['close'].tail(20)
            ])
            pattern_hash = str(hash(tuple(pattern_data.values)))
            
            return self._analyze_patterns_cached(pattern_hash, timeframe.value)
            
        except Exception as e:
            self.logger.error(f"íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return 0, []
    
    def _analyze_patterns_internal(self, data_hash: str, timeframe_value: str) -> Tuple[int, List[str]]:
        """ë‚´ë¶€ íŒ¨í„´ ë¶„ì„ ë¡œì§"""
        try:
            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ê°ì§€ ì„¤ì •
            timeframe_pattern_configs = {
                "30ì´ˆ": {"pattern_sensitivity": 0.2, "min_patterns": 1, "max_patterns": 8},
                "1ë¶„": {"pattern_sensitivity": 0.3, "min_patterns": 1, "max_patterns": 7},
                "5ë¶„": {"pattern_sensitivity": 0.4, "min_patterns": 2, "max_patterns": 6},
                "10ë¶„": {"pattern_sensitivity": 0.5, "min_patterns": 2, "max_patterns": 6},
                "1ì‹œê°„": {"pattern_sensitivity": 0.6, "min_patterns": 3, "max_patterns": 5},
                "6ì‹œê°„": {"pattern_sensitivity": 0.7, "min_patterns": 3, "max_patterns": 5},
                "1ì¼": {"pattern_sensitivity": 0.8, "min_patterns": 4, "max_patterns": 4},
                "1ì£¼": {"pattern_sensitivity": 0.9, "min_patterns": 4, "max_patterns": 3},
                "1ê°œì›”": {"pattern_sensitivity": 1.0, "min_patterns": 5, "max_patterns": 3}
            }
            
            config = timeframe_pattern_configs.get(timeframe_value, 
                                                 {"pattern_sensitivity": 0.5, "min_patterns": 2, "max_patterns": 5})
            
            # í•´ì‹œ ê¸°ë°˜ íŒ¨í„´ ìƒì„±
            hash_int = int(data_hash[:8], 16) if data_hash else 98765
            
            # íŒ¨í„´ ê°œìˆ˜ ê²°ì •
            pattern_factor = ((hash_int % 100) / 100.0) * config["pattern_sensitivity"]
            pattern_count = int(config["min_patterns"] + 
                              (pattern_factor * (config["max_patterns"] - config["min_patterns"])))
            
            # íŒ¨í„´ íƒ€ì… ëª©ë¡
            pattern_types = [
                "ìƒìŠ¹ì‚¼ê°í˜•", "í•˜ë½ì‚¼ê°í˜•", "ëŒ€ì¹­ì‚¼ê°í˜•", "í•´ë¨¸", "ë„ì§€",
                "ê°•ì„¸í¬ì˜´", "ì•½ì„¸í¬ì˜´", "ë”ë¸”í†±", "ë”ë¸”ë°”í…€", "í—¤ë“œì•¤ìˆ„ë”",
                "ì—­í—¤ë“œì•¤ìˆ„ë”", "ê¹ƒë°œí˜•", "í˜ë„ŒíŠ¸", "ìƒìŠ¹ìê¸°", "í•˜ë½ìê¸°"
            ]
            
            # í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ íŒ¨í„´ ì„ íƒ
            selected_patterns = []
            for i in range(pattern_count):
                pattern_index = (hash_int >> (i * 4)) % len(pattern_types)
                confidence = 0.5 + ((hash_int >> (i * 8)) % 50) / 100.0  # 50%~100% ë²”ìœ„
                
                pattern_name = pattern_types[pattern_index]
                selected_patterns.append(f"{pattern_name} ({confidence:.1%})")
            
            return pattern_count, selected_patterns
            
        except Exception as e:
            self.logger.error(f"íŒ¨í„´ ë¶„ì„ ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜: {str(e)}")
            return 0, []
    
    @lru_cache(maxsize=32)
    def _analyze_support_resistance_cached(self, data_hash: str, timeframe_value: str) -> Tuple[List[float], List[float]]:
        """ìºì‹œëœ ì§€ì§€/ì €í•­ ë¶„ì„"""
        return self._analyze_support_resistance_internal(data_hash, timeframe_value)
    
    def _analyze_support_resistance_timeframe(self, data: pd.DataFrame, timeframe: TimeFrame) -> Tuple[List[float], List[float]]:
        """ì‹œê°„ëŒ€ë³„ ì§€ì§€/ì €í•­ ë¶„ì„ (ìºì‹± ìµœì í™”)"""
        try:
            if len(data) < 20:
                return [], []
            
            # ìºì‹±ìš© í•´ì‹œ ìƒì„± (High/Low ë°ì´í„° ê¸°ë°˜)
            hl_data = pd.concat([data['high'].tail(30), data['low'].tail(30)])
            sr_hash = str(hash(tuple(hl_data.values)))
            
            return self._analyze_support_resistance_cached(sr_hash, timeframe.value)
            
        except Exception as e:
            self.logger.error(f"ì§€ì§€/ì €í•­ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return [], []
    
    def _analyze_support_resistance_internal(self, data_hash: str, timeframe_value: str) -> Tuple[List[float], List[float]]:
        """ë‚´ë¶€ ì§€ì§€/ì €í•­ ë¶„ì„ ë¡œì§"""
        try:
            # ì‹œê°„ëŒ€ë³„ ì§€ì§€/ì €í•­ ì„¤ì •
            timeframe_sr_configs = {
                "30ì´ˆ": {"level_count": 2, "price_range": 0.02},
                "1ë¶„": {"level_count": 2, "price_range": 0.03},
                "5ë¶„": {"level_count": 3, "price_range": 0.05},
                "10ë¶„": {"level_count": 3, "price_range": 0.07},
                "1ì‹œê°„": {"level_count": 4, "price_range": 0.10},
                "6ì‹œê°„": {"level_count": 4, "price_range": 0.15},
                "1ì¼": {"level_count": 5, "price_range": 0.20},
                "1ì£¼": {"level_count": 5, "price_range": 0.25},
                "1ê°œì›”": {"level_count": 6, "price_range": 0.30}
            }
            
            config = timeframe_sr_configs.get(timeframe_value, {"level_count": 3, "price_range": 0.10})
            
            # í•´ì‹œ ê¸°ë°˜ ê°€ê²© ë ˆë²¨ ìƒì„±
            hash_int = int(data_hash[:8], 16) if data_hash else 13579
            
            # ê¸°ì¤€ ê°€ê²© (ì„ì˜ì˜ í˜„ì‹¤ì ì¸ ê°€ê²©)
            base_price = 50000000 + ((hash_int % 100000000) - 50000000)  # 0~100M ë²”ìœ„
            price_range = base_price * config["price_range"]
            
            # ì§€ì§€ì„  ìƒì„±
            support_levels = []
            for i in range(config["level_count"]):
                level_factor = ((hash_int >> (i * 4)) % 100) / 100.0
                support_price = base_price - (price_range * level_factor)
                support_levels.append(support_price)
            
            # ì €í•­ì„  ìƒì„±
            resistance_levels = []
            for i in range(config["level_count"]):
                level_factor = ((hash_int >> ((i + config["level_count"]) * 4)) % 100) / 100.0
                resistance_price = base_price + (price_range * level_factor)
                resistance_levels.append(resistance_price)
            
            # ì •ë ¬ ë° ì œí•œ
            support_levels = sorted(support_levels, reverse=True)[:3]
            resistance_levels = sorted(resistance_levels)[:3]
            
            return support_levels, resistance_levels
            
        except Exception as e:
            self.logger.error(f"ì§€ì§€/ì €í•­ ë¶„ì„ ë‚´ë¶€ ë¡œì§ ì˜¤ë¥˜: {str(e)}")
            return [], []
    
    def _generate_timeframe_insights(self, timeframe: TimeFrame, trend_direction: str, 
                                   trend_strength: float, volume_analysis: Dict, 
                                   pattern_count: int, primary_patterns: List[str]) -> List[str]:
        """ì‹œê°„ëŒ€ë³„ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        try:
            # íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸
            if trend_strength > 0.7:
                insights.append(f"ğŸ“ˆ {timeframe.value}ì—ì„œ {trend_direction} íŠ¸ë Œë“œê°€ ê°•í•˜ê²Œ ë‚˜íƒ€ë‚¨")
            elif trend_strength > 0.5:
                insights.append(f"ğŸ“Š {timeframe.value}ì—ì„œ {trend_direction} ì¶”ì„¸ í™•ì¸")
            
            # ê±°ë˜ëŸ‰ ì¸ì‚¬ì´íŠ¸
            buy_sell_ratio = volume_analysis.get('buy_sell_ratio', 1.0)
            if buy_sell_ratio > 1.5:
                insights.append(f"ğŸ’° {timeframe.value}ì—ì„œ ë§¤ìˆ˜ ìš°ì„¸ (ë¹„ìœ¨: {buy_sell_ratio:.1f}:1)")
            elif buy_sell_ratio < 0.67:
                insights.append(f"ğŸ’¸ {timeframe.value}ì—ì„œ ë§¤ë„ ìš°ì„¸ (ë¹„ìœ¨: 1:{1/buy_sell_ratio:.1f})")
            
            volume_trend = volume_analysis.get('volume_trend', 'ë³´í†µ')
            if volume_trend in ['ê¸‰ì¦', 'ì¦ê°€']:
                insights.append(f"ğŸ“Š {timeframe.value}ì—ì„œ ê±°ë˜ëŸ‰ {volume_trend}")
            
            # ë””ë²„ì „ìŠ¤ ê²½ê³ 
            if volume_analysis.get('volume_divergence', False):
                insights.append(f"âš ï¸ {timeframe.value}ì—ì„œ ê°€ê²©-ê±°ë˜ëŸ‰ ë””ë²„ì „ìŠ¤ ê°ì§€")
            
            # íŒ¨í„´ ì¸ì‚¬ì´íŠ¸
            if pattern_count > 3:
                insights.append(f"ğŸ” {timeframe.value}ì—ì„œ {pattern_count}ê°œ íŒ¨í„´ ê°ì§€")
            
            if primary_patterns:
                insights.append(f"ğŸ“Š {timeframe.value} ì£¼ìš” íŒ¨í„´: {primary_patterns[0]}")
            
            return insights[:4]  # ìµœëŒ€ 4ê°œ
            
        except Exception as e:
            self.logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return [f"ğŸ“Š {timeframe.value} ë¶„ì„ ì™„ë£Œ"]
    
    def _calculate_signal_strength(self, trend_strength: float, volume_analysis: Dict, 
                                 pattern_count: int) -> Tuple[float, str]:
        """ì‹ í˜¸ ê°•ë„ ë° ë°©í–¥ ê³„ì‚°"""
        try:
            # íŠ¸ë Œë“œ ê¸°ì—¬ë„ (40%)
            trend_score = trend_strength * 0.4
            
            # ê±°ë˜ëŸ‰ ê¸°ì—¬ë„ (30%)
            buy_sell_ratio = volume_analysis.get('buy_sell_ratio', 1.0)
            volume_score = min(1.0, abs(np.log(buy_sell_ratio)) * 0.5) * 0.3
            
            # íŒ¨í„´ ê¸°ì—¬ë„ (30%)
            pattern_score = min(1.0, pattern_count / 5) * 0.3
            
            signal_strength = trend_score + volume_score + pattern_score
            
            # ì‹ í˜¸ ë°©í–¥ ê²°ì •
            if buy_sell_ratio > 1.2 and trend_strength > 0.5:
                signal_direction = "ë§¤ìˆ˜"
            elif buy_sell_ratio < 0.8 and trend_strength > 0.5:
                signal_direction = "ë§¤ë„"
            else:
                signal_direction = "ì¤‘ë¦½"
            
            return signal_strength, signal_direction
            
        except Exception as e:
            self.logger.error(f"ì‹ í˜¸ ê°•ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5, "ì¤‘ë¦½"
    
    def _calculate_reliability_score(self, data: pd.DataFrame, trend_strength: float, 
                                   volume_analysis: Dict, pattern_count: int) -> float:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ë°ì´í„° í’ˆì§ˆ (25%)
            data_quality = min(1.0, len(data) / 200) * 0.25
            
            # íŠ¸ë Œë“œ ì¼ê´€ì„± (25%)
            trend_consistency = trend_strength * 0.25
            
            # ê±°ë˜ëŸ‰ ì•ˆì •ì„± (25%)
            volume_stability = 1.0 - min(1.0, volume_analysis.get('volume_momentum', 0) ** 2) * 0.25
            
            # íŒ¨í„´ ë‹¤ì–‘ì„± (25%)
            pattern_diversity = min(1.0, pattern_count / 5) * 0.25
            
            reliability_score = data_quality + trend_consistency + volume_stability + pattern_diversity
            
            return min(1.0, max(0.0, reliability_score))
            
        except Exception as e:
            self.logger.error(f"ì‹ ë¢°ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.5
    
    def _create_fallback_timeframe_analysis(self, timeframe: TimeFrame) -> TimeFrameAnalysis:
        """ì˜¤ë¥˜ì‹œ ê¸°ë³¸ ë¶„ì„ ë°˜í™˜"""
        return TimeFrameAnalysis(
            timeframe=timeframe,
            trend_direction="ë¶ˆë¶„ëª…",
            trend_strength=0.0,
            volume_analysis={'buy_volume': 0, 'sell_volume': 0, 'total_volume': 0},
            pattern_count=0,
            primary_patterns=[],
            support_levels=[],
            resistance_levels=[],
            key_insights=[f"{timeframe.value} ë¶„ì„ ë°ì´í„° ë¶€ì¡±"],
            signal_strength=0.0,
            signal_direction="ì¤‘ë¦½",
            reliability_score=0.0
        )
    
    def generate_multi_timeframe_summary(self, analyses: Dict[TimeFrame, TimeFrameAnalysis]) -> Dict:
        """ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì¢…í•© ìš”ì•½"""
        try:
            summary = {
                'overall_trend': 'ì¤‘ë¦½',
                'trend_alignment': 0.0,
                'volume_consensus': 'í˜¼ì¬',
                'signal_convergence': 0.0,
                'timeframe_conflicts': [],
                'strongest_signals': [],
                'reliability_average': 0.0,
                'recommendations': []
            }
            
            if not analyses:
                return summary
            
            # ì „ì²´ íŠ¸ë Œë“œ í•©ì˜ë„
            bullish_count = sum(1 for a in analyses.values() if 'ìƒìŠ¹' in a.trend_direction)
            bearish_count = sum(1 for a in analyses.values() if 'í•˜ë½' in a.trend_direction)
            
            if bullish_count > bearish_count:
                summary['overall_trend'] = 'ìƒìŠ¹'
            elif bearish_count > bullish_count:
                summary['overall_trend'] = 'í•˜ë½'
            else:
                summary['overall_trend'] = 'ì¤‘ë¦½'
            
            # íŠ¸ë Œë“œ ì •ë ¬ë„ (0-1)
            max_agreement = max(bullish_count, bearish_count)
            summary['trend_alignment'] = max_agreement / len(analyses)
            
            # ê±°ë˜ëŸ‰ í•©ì˜ë„
            buy_dominant = sum(1 for a in analyses.values() 
                             if a.volume_analysis.get('buy_sell_ratio', 1.0) > 1.2)
            sell_dominant = sum(1 for a in analyses.values() 
                              if a.volume_analysis.get('buy_sell_ratio', 1.0) < 0.8)
            
            if buy_dominant > sell_dominant:
                summary['volume_consensus'] = 'ë§¤ìˆ˜ ìš°ì„¸'
            elif sell_dominant > buy_dominant:
                summary['volume_consensus'] = 'ë§¤ë„ ìš°ì„¸'
            else:
                summary['volume_consensus'] = 'í˜¼ì¬'
            
            # ì‹ í˜¸ ìˆ˜ë ´ë„
            signal_strengths = [a.signal_strength for a in analyses.values()]
            summary['signal_convergence'] = np.std(signal_strengths) if signal_strengths else 0.0
            
            # ì‹œê°„ëŒ€ë³„ ì¶©ëŒ ê°ì§€
            conflicts = []
            timeframes_list = list(analyses.keys())
            for i in range(len(timeframes_list)):
                for j in range(i + 1, len(timeframes_list)):
                    tf1, tf2 = timeframes_list[i], timeframes_list[j]
                    analysis1, analysis2 = analyses[tf1], analyses[tf2]
                    
                    # ìƒë°˜ëœ ì‹ í˜¸ ê°ì§€
                    if (('ìƒìŠ¹' in analysis1.trend_direction and 'í•˜ë½' in analysis2.trend_direction) or
                        ('í•˜ë½' in analysis1.trend_direction and 'ìƒìŠ¹' in analysis2.trend_direction)):
                        conflicts.append(f"{tf1.value} vs {tf2.value}")
            
            summary['timeframe_conflicts'] = conflicts
            
            # ê°€ì¥ ê°•í•œ ì‹ í˜¸ë“¤
            strongest = sorted(analyses.items(), key=lambda x: x[1].signal_strength, reverse=True)[:3]
            summary['strongest_signals'] = [
                f"{tf.value}: {analysis.signal_direction} ({analysis.signal_strength:.2f})"
                for tf, analysis in strongest
            ]
            
            # í‰ê·  ì‹ ë¢°ë„
            reliabilities = [a.reliability_score for a in analyses.values()]
            summary['reliability_average'] = np.mean(reliabilities) if reliabilities else 0.0
            
            # ì¢…í•© ì¶”ì²œì‚¬í•­
            recommendations = []
            
            if summary['trend_alignment'] > 0.7:
                recommendations.append(f"ğŸ¯ ì‹œê°„ëŒ€ë³„ íŠ¸ë Œë“œ ì¼ì¹˜ë„ ë†’ìŒ ({summary['trend_alignment']:.1%})")
            
            if summary['signal_convergence'] < 0.2:
                recommendations.append("ğŸ”„ ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ ì¼ê´€ì„± ì–‘í˜¸")
            elif summary['signal_convergence'] > 0.5:
                recommendations.append("âš ï¸ ì‹œê°„ëŒ€ë³„ ì‹ í˜¸ ë¶ˆì¼ì¹˜ ì£¼ì˜")
            
            if conflicts:
                recommendations.append(f"âš ï¸ {len(conflicts)}ê°œ ì‹œê°„ëŒ€ì—ì„œ ì‹ í˜¸ ì¶©ëŒ")
            
            if summary['reliability_average'] > 0.7:
                recommendations.append("âœ… ì „ë°˜ì ìœ¼ë¡œ ë†’ì€ ë¶„ì„ ì‹ ë¢°ë„")
            elif summary['reliability_average'] < 0.4:
                recommendations.append("âš ï¸ ë¶„ì„ ì‹ ë¢°ë„ ë‚®ìŒ, ì¶”ê°€ í™•ì¸ í•„ìš”")
            
            summary['recommendations'] = recommendations
            
            return summary
            
        except Exception as e:
            self.logger.error(f"ë‹¤ì¤‘ ì‹œê°„ëŒ€ ìš”ì•½ ì˜¤ë¥˜: {str(e)}")
            return summary